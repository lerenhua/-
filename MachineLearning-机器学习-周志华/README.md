# Manchine Learning Reading Summary
此repo总结记录周志华著作《机器学习》的内容，用于之后复习深入。 
# 目录
* [chapter 1](#chapter-1)：绪论
* [chapter 2](#chapter-2)：模型评估与选择
# 章节内容
## chapter 1
#### 基本术语
* 数据集：示例（instance）或样本（sample）的集合。
* 属性或特征：可以理解为事件或对象在某方面的表现或性质的事项，也就是可以描述体现该对象某方面特点的维度。
* 属性值或特征值：属性或特征上的取值。
* 属性空间，样本空间或输入空间：可以理解为这样一个空间，所有的特征构成描绘这个空间维度坐标，各个特征的取值为各个维度的取值范围，每个样本都可以在该空间中找到唯一位置。
* 特征向量：样本空间中每个点对应一个坐标向量，即为特征向量。
* 假设（hypothesis）：模型学习到的关于数据的某种潜在规律。
* 真相（ground-truth）：数据本身潜在的规律，可以理解为是客观事实。
#### 假设空间
* 可以将学习的过程看作是在所有假设（hypothesis）组成的空间中进行搜索的过程，搜索目标是找出与训练集相适应的假设。
#### 归纳偏好
* 学习算法本身的偏好会使得最后学习到的模型有不同侧重点，即对某种类型的假设有更多的偏好
* 学习算法必须具有归纳偏好，这样学习到的模型能产生确定的结果。
* 归纳偏好对应了学习算法本身认为“什么样的模型更好”的假设。因此，大多数时候算法是否取得好的性能取决于算法的归纳偏好是否与实际问题相匹配。

## chapter 2
#### 模型评估与选择的三个关键问题
* 如何获得测试结果，即``评估方法``有哪些？
* 如何评估性能优劣，即``性能度量``有些方式？
* 如何判断实质差别，即``比较检验``如何进行？

#### 评估方法
* 通常可以通过实验测试来对学习器泛化误差进行评估和选择，为此需要``测试集``来测试学习器对新样本的判别能力，然后以测试误差作为泛化误差的近似。
* 评估方法的关键在于怎么获得``测试集``
* ``留出法``
  * 直接将数据集 $D$ 划分为两个互斥的集合，其中一个集合作为训练集 $S$ ,另一个作为测试集 $T$ ，即 $D = S \bigcup T, S \bigcap T = \emptyset$。在 $S$ 上训练出模型后，用 $T$ 来评估其训练测试误差，作为泛化误差的估计。![留出法](images/1.png)
  * 注意：
    * 保持数据分布一致性 （例如：分层采样）
    * 多次重复划分（例如：100次随机划分）
    * 测试集不能太大，不能太小（例如：1/5~1/3）
* ``交叉验证法``
  * 将数据集 $D$ 划分为k个大小相似的互斥子集，即 $D = D_1 \bigcup D_2 \bigcup \cdots \bigcup D_k, D_i \bigcap D_j = \emptyset (i \neq j)$。每个子集 $D_i$ 都尽可能保持数据分布的一致性。然后，每次用 k-1 个子集的并集作为训练集，余下的那个子集作为测试集；这样就可以获取 k 组训练测试集，最终返回的是这 k 个测试结果的均值。![10折交叉验证](images/2.png)
* ``自助法``
  * 用于减少训练样本规模变化不同造成的影响，同时还能比较高效地进行实验估计。
  * 给定包含 $m$ 个样本的数据集 $D$ ，我们对其进行采样产生数据集 $D'$:每次随机从 $D$ 中挑选一个样本拷贝入 $D'$，然后再将样本放回原数据集，重复执行 $m$ 次。![](images/3.png)
  * 通过自助采样，初始数据集 $D$ 中约有36.8%的样本未出现在采样数据集 $D'$ 中。于是将 $D'$ 用作训练集，将 $D \setminus D'$(集合减法)作为测试集。
  * 自助法在数据集较小，难以有效划分训练测试集时很有用；此外，自助法能从初始数据集中产生多个不同训练集，可以用于集成学习。但数据样本充足时，留出法和交叉验证法更常用一些。
* 调参与最终模型
  * 学习算法的超参数选择与模型参数的学习无本质区别
  * 一般通过添加验证集，来评估超参数的结果。
#### 性能度量
* ``性能度量``是衡量模型泛化能力的评价标准，反映了任务需求。使用不同的性能度量往往会导致不同的评判结果。``什么样的模型是好的，不仅取决于算法和数据，还取决于任务需求``。
* 回归任务常用均方误差：
  $$E(f;D) = \frac{1}{m}\sum_{i=1}^m(f(x_i) - y_i)^2$$
#### 比较检验
* 有了实验评估方法和性能度量，接下来便是如何比较结果。影响比较结果的重要因素：
  * 我们需要比较泛化性能，然而通过实验评估的方法我们获取的是测试集上的性能，二者对比结果可能不相同
  * 测试集上性能与测试集本身的选择有很大关系，且无论使用不同大小的测试集会得到不同的结果，即便是相同大小的测试集若包含测试样例不同，其测试结果也会不同
  * 很多机器学习算法本身有一定的随机性，即便用相同的参数设置同一个测试集上多次运行，其结果也会有不同
* 基于统计假设检验结果我们可以推断出，若在测试集上观察到学习器A比B好，则A的泛化性能是否在统计意义上优于B，以及这个结论的把握有多大。
* ``假设检验``

* 两学习器比较
  * ``交叉验证 t 检验``
  * ``McNemar 检验``
* 多学习器比较
  * ``Friedman 检验``
  * ``Nemenyi 检验``