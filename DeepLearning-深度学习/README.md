# deep learning book record
此repo用于记录总结 Ian Goodfellow and Yoshua Bengio and Aaron Courville等著作的《深度学习》内容，网址：http://www.deeplearningbook.org/
<br> 如需显示数学公式，谷歌浏览器可安装插件：https://chrome.google.com/webstore/detail/github-with-mathjax/ioemnmodlmafdkllaclgeombjnmnbima
# 目录
* [chapter 3](#chapter-3)：概率与信息论
* [chapter 4](#chapter-4)：数值计算

# 章节内容
## chapter 2
### 标量，向量，矩阵和张量
### 线性相关和生成子空间
* 为分析方程有多少个解，可以将 $A$ 的列向量看作从原点（元素都是零的向量）出发的不同方向，确定有多少种方法可以到达向量 $b$ 。此种观点下，认为 $n$ 维列向量为 $n$ 维空间中的一点，列向量的个数为初始方向的个数。
* 向量 $x$ 中的每个元素表示应该沿各方向走多远，即 $x_i$ 表示我们需要沿着第 $i$ 个向量的方向走多远：
  $$
  Ax = \sum_ix_iA_{:, i}
  $$
  其中， $A_{:, i}$为第 $i$ 个列向量。一般这个操作为``线性组合``。
* 一组向量的``生成子空间``（列空间）是原始向量线性组合后所能到达的点的集合。如列向量为3维，而列向量个数为2，则最多可以描述3维空间的2维平面。因此， $Ax = b$ 要有解，则充分条件为列向量个数大于列向量维数。
* 如果向量组个数大于向量维度，但列空间无法充满向量维度空间，则向量组``线性相关``。
* 如果一组向量组中任意一个向量都不能表示成其他向量的线性组合，那么向量组``线性无关``。
* 对于 $m * n$ 矩阵可逆，则最多只有 $m$ 个列向量线性无关，因此该矩阵为``方阵``。
* ``奇异矩阵`` 指列向量线性相关的方阵。
### 范数
* 在机器学习中，常用范数（norm）的函数衡量向量大小。
* $L^p$ 范数定义：
  $$
  ||x||_p = (\sum_i|x_i|^p)^{\frac{1}{p}}
  $$
  其中$p\in{\mathbb{R}}$, $p\geq1$。
* 范数是将向量映射到非负值的函数。直观说，向量 $x$ 的范数衡量从原点到 $x$ 的距离。
* 常用范数
  * $L^2$范数被称为欧几里得范数。它表示从原点出发到向量 $x$ 确定的点的欧几里得距离。
  * $L^1$ 范数：$||x||_1 = \sum_i|x_i|$
  * $L^\infty$范数，也称最大范数（max norm）。这个范数表示向量中具有最大幅值的元素的绝对值：
    $$
    ||x||_\infty = \max_i|x_i|
    $$
### 特殊类型的矩阵和向量
* 对角矩阵（diagonal matrix）：只在主对角线上含有非零元素，其他位置为零。不是所有的对角矩阵都是方阵。
* 对称矩阵（symmetric matrix)是转置与自己相等的矩阵：
  $$
  A = A^T
  $$
  对称矩阵是方阵。
* 单位向量（unit vector）是具有单位范数（unit norm）的向量：
  $$
  ||x||_2 = 1
  $$
* 如果 $x^Ty = 0$ ，那么向量 $x$ 和向量 $y$ 互相正交（orthogonal）。如果两个向量都为非零范数，那么这两个向量之间的夹角为90度。在 $\mathbb{R}^n$ 中，至多有 $n$ 个范数非零向量正交。如果这些向量不仅互相正交，并且范数都为1，那么称他们是标准正交。
* 正交矩阵（orthogonal matrix）是指行向量和列向量是分别标准正交的矩阵：
  $$
  A^TA = AA^T = I
  $$
  也就是
  $$
  A^{-1} = A^T
  $$
### 特征分解
* 通过分解矩阵可以找出一些不易发现的内在性质。特征分解（eigendecomposition）是使用最广的矩阵分解之一，将矩阵分解成一组特征向量和特征值。
* 方阵 $A$ 的特征向量（eigen vector）是指与 $A$ 相乘后相对于对该向量进行缩放的非零向量 $v$ :
  $$
  Av = \lambda v
  $$
  其中，标量 $\lambda$ 为该特征向量对应的特征值。
* 假设矩阵 $A$ 有 $n$ 个线性无关的特征向量 $\{v^{(1)}, \cdots, v^{(n)} \}$ ， 对应特征值 $\{\lambda_1, \cdots, \lambda_n \}$。将特征向量连接成一个矩阵，每一列是一个特征向量： $V = [v^{(1)}, \cdots, v^{(n)} ]$ ，将特征值连接成一个向量： $\mathbb{\lambda} = [\lambda_1, \cdots, \lambda_n ]^T$。因此 $A$ 的特征分解表示为
  $$
  A = Vdiag(\lambda)V^{-1}
  $$
  其中， $diag(\lambda)$ 表示生成的对角方阵。
* 实对称矩阵都可以分解为实特征向量和实特征值：
  $$
  A = Q\Lambda Q^T
  $$
  其中 $Q$ 是 $A$ 的特征向量构成的正交矩阵， $\Lambda$ 是对角矩阵。
* ``正定矩阵``指所有特征值都是正数的矩阵；``半正定矩阵``指所有特征值都是非负数的矩阵；``负定矩阵``指所有特征值都是负数的矩阵；``半负定矩阵``指所有特征值都是非正数的矩阵。
### 奇异值分解
* 奇异值分解（singular value decomposition， SVD）将矩阵分解为奇异向量（singular vector）和奇异值（singular value）。每一个矩阵都有奇异值分解，但不一定有特征分解。如非方阵的矩阵没有特征分解，只能使用奇异值分解。

## chapter 3
### 概率分布

### 期望，方程与协方差
### 结构化概率模型
* 机器学习算法会涉及到非常多的随机变量上的概率分布。单函数描述整个联合概率分布是非常低效的。利用分解可以减少表示联合分布的成本。可以利用图来表示概率分布的分解，称为结构化概率模型（structured probabilistic model）或者图模型（graphical model）。

## chapter 4
### 下溢和上溢
* 下溢（underflow）：当数很小接近零时，由于计算机系统保存精度的问题会使得数被四舍五入变成零。此时软件环境可能会抛出异常或返回非数字（NaN）的占位符。
* 上溢（overflow）：当数非常大以至于超过可表示数的上限时，数被近似为 $\infty$ 。
* 为了确保数值计算的正确性，所以需要考虑数值稳定性问题，防止发生上溢或下溢。
### 病态条件
* ``条件数``表征函数相对于输入的微小变化而变化的快慢程度。如果函数因输入轻微扰动而迅速变化，这对于科学计算而言可能有问题，输入中的舍入误差可能导致输出巨大的变化。
* 考虑函数 $f(x) = A^{-1}x$ 。当 $A \in \mathbb{R}^{n * n}$ 具有特征值分解时，其条件数为
  $$
  \max_{i, j}|\frac{\lambda_i}{\lambda_j}|
  $$
### 基于梯度的优化方法
* 在深度学习中，通常优化具有多维输入的函数：$f:\mathbb{R}^n \longrightarrow \mathbb{R}$ ,其输出为一维（标量）。
* 
### 约束优化
* 有时候在所有 $x$ 的所有可能值下最大化或最小化一个函数 $f(x)$ 不是我们所需要的。相反希望在 $x$ 的某些集合 $\mathbb{S}$ 中找 $f(x)$ 的最大值或最小值，这个过程为``约束优化``(constrained optimization)。通常，会在损失函数上加一个范数约束。

## chapter 5
### 学习算法
* 任务 $T$
* 性能度量 $P$
* 经验 $E$
### 超参数与验证集
* 交叉验证